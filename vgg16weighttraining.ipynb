{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\major_project\\env_major_project\\lib\\site-packages\\torch\\nn\\functional.py:3103: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
      "d:\\major_project\\env_major_project\\lib\\site-packages\\torch\\nn\\functional.py:3103: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
      "d:\\major_project\\env_major_project\\lib\\site-packages\\torch\\nn\\functional.py:3103: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
      "d:\\major_project\\env_major_project\\lib\\site-packages\\torch\\nn\\functional.py:3103: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
      "d:\\major_project\\env_major_project\\lib\\site-packages\\torch\\nn\\functional.py:3103: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
      "d:\\major_project\\env_major_project\\lib\\site-packages\\torch\\nn\\functional.py:3103: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
      "d:\\major_project\\env_major_project\\lib\\site-packages\\torch\\nn\\functional.py:3103: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
      "d:\\major_project\\env_major_project\\lib\\site-packages\\torch\\nn\\functional.py:3103: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
      "d:\\major_project\\env_major_project\\lib\\site-packages\\torch\\nn\\functional.py:3103: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
      "d:\\major_project\\env_major_project\\lib\\site-packages\\torch\\nn\\functional.py:3103: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
      "d:\\major_project\\env_major_project\\lib\\site-packages\\torch\\nn\\functional.py:3103: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
      "d:\\major_project\\env_major_project\\lib\\site-packages\\torch\\nn\\functional.py:3103: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['im_Dyskeratotic', 'im_Koilocytotic', 'im_Metaplastic', 'im_Parabasal', 'im_Superficial-Intermediate']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\major_project\\env_major_project\\lib\\site-packages\\torch\\nn\\functional.py:3103: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg= models.vgg16_bn\n",
    "#hyperparameters\n",
    "batch_size = 10\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "#Storing path\n",
    "save_loc = 'vgg16model_trainedonSIPAKMEDdataset5000' + str(epochs) + \"batch\" + str(batch_size)\n",
    "\n",
    "## Declaring path of dataset\n",
    "path_img = Path(\"SIPakMed_format\")\n",
    "\n",
    "#Declaring the .pth path for the model weights\n",
    "weights_path = path_img/'models2'/\"vgg16modeltrainedon_DA_HERLEV_SIPAKMED95accuracy\"/\"vgg16model_trainedonExtendedDAdataset50batch100.001BEST\"  #this needs to be of .pth extension\n",
    "\n",
    "#Model path (.pkl) to the folder with the \"export.pkl\" seraialization file\n",
    "model_path = path_img/'models2'/\"vgg16SimplesipakmedE10B5accuracy87\"   #this needs to be of .pkl extension and it needs to have the name \"export.pkl\"\n",
    "## Loading data \n",
    "data = ImageDataBunch.from_folder(path=path_img, train='train',\n",
    "            valid='val', ds_tfms=get_transforms(), size = 224, bs=batch_size)#, check_ext=False)  #the size of the input pictures is quite important\n",
    "## Normalizing data based on Image net parameters\n",
    "data.normalize(imagenet_stats)\n",
    "#data.show_batch(rows=3, figsize=(10,8))\n",
    "print(data.classes)\n",
    "len(data.classes),data.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\" to C:\\Users\\Aditya Arora/.cache\\torch\\hub\\checkpoints\\vgg16_bn-6c64b313.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daab7040292c415c8da7508141c16490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/528M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      50.00% [1/2 05:44<05:44]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f_beta</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.312576</td>\n",
       "      <td>#na#</td>\n",
       "      <td>05:44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='31' class='' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      54.39% [31/57 02:43<02:16 8.5534]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f_beta</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.025729</td>\n",
       "      <td>1.072493</td>\n",
       "      <td>0.618557</td>\n",
       "      <td>0.613371</td>\n",
       "      <td>06:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.532469</td>\n",
       "      <td>0.665466</td>\n",
       "      <td>0.752577</td>\n",
       "      <td>0.750943</td>\n",
       "      <td>06:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.207045</td>\n",
       "      <td>0.542950</td>\n",
       "      <td>0.783505</td>\n",
       "      <td>0.781357</td>\n",
       "      <td>07:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.963810</td>\n",
       "      <td>0.435277</td>\n",
       "      <td>0.845361</td>\n",
       "      <td>0.845157</td>\n",
       "      <td>06:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.695302</td>\n",
       "      <td>0.403363</td>\n",
       "      <td>0.855670</td>\n",
       "      <td>0.855838</td>\n",
       "      <td>06:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.641765</td>\n",
       "      <td>0.303106</td>\n",
       "      <td>0.907216</td>\n",
       "      <td>0.906723</td>\n",
       "      <td>06:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.585165</td>\n",
       "      <td>0.358176</td>\n",
       "      <td>0.891753</td>\n",
       "      <td>0.891309</td>\n",
       "      <td>06:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.653522</td>\n",
       "      <td>0.648230</td>\n",
       "      <td>0.855670</td>\n",
       "      <td>0.853911</td>\n",
       "      <td>06:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.522252</td>\n",
       "      <td>0.536018</td>\n",
       "      <td>0.876289</td>\n",
       "      <td>0.875682</td>\n",
       "      <td>05:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.495865</td>\n",
       "      <td>0.508107</td>\n",
       "      <td>0.855670</td>\n",
       "      <td>0.854303</td>\n",
       "      <td>05:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.603100</td>\n",
       "      <td>0.987139</td>\n",
       "      <td>0.824742</td>\n",
       "      <td>0.822955</td>\n",
       "      <td>06:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.629673</td>\n",
       "      <td>0.764519</td>\n",
       "      <td>0.742268</td>\n",
       "      <td>0.736365</td>\n",
       "      <td>05:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.655141</td>\n",
       "      <td>0.928314</td>\n",
       "      <td>0.798969</td>\n",
       "      <td>0.793374</td>\n",
       "      <td>05:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.619829</td>\n",
       "      <td>0.404008</td>\n",
       "      <td>0.886598</td>\n",
       "      <td>0.885085</td>\n",
       "      <td>06:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.552474</td>\n",
       "      <td>0.674686</td>\n",
       "      <td>0.814433</td>\n",
       "      <td>0.806680</td>\n",
       "      <td>06:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.597093</td>\n",
       "      <td>1.553592</td>\n",
       "      <td>0.654639</td>\n",
       "      <td>0.634017</td>\n",
       "      <td>06:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.672205</td>\n",
       "      <td>0.882500</td>\n",
       "      <td>0.783505</td>\n",
       "      <td>0.774264</td>\n",
       "      <td>06:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.566332</td>\n",
       "      <td>0.380198</td>\n",
       "      <td>0.865979</td>\n",
       "      <td>0.865194</td>\n",
       "      <td>07:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.457274</td>\n",
       "      <td>0.545795</td>\n",
       "      <td>0.865979</td>\n",
       "      <td>0.864377</td>\n",
       "      <td>06:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.547250</td>\n",
       "      <td>0.595755</td>\n",
       "      <td>0.814433</td>\n",
       "      <td>0.810036</td>\n",
       "      <td>06:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.633068</td>\n",
       "      <td>0.364438</td>\n",
       "      <td>0.886598</td>\n",
       "      <td>0.885811</td>\n",
       "      <td>06:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.505949</td>\n",
       "      <td>0.622026</td>\n",
       "      <td>0.829897</td>\n",
       "      <td>0.816561</td>\n",
       "      <td>06:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.455791</td>\n",
       "      <td>0.268805</td>\n",
       "      <td>0.917526</td>\n",
       "      <td>0.917237</td>\n",
       "      <td>06:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.421771</td>\n",
       "      <td>0.518620</td>\n",
       "      <td>0.809278</td>\n",
       "      <td>0.806033</td>\n",
       "      <td>06:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.364148</td>\n",
       "      <td>0.326219</td>\n",
       "      <td>0.896907</td>\n",
       "      <td>0.895369</td>\n",
       "      <td>06:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.352336</td>\n",
       "      <td>0.308377</td>\n",
       "      <td>0.881443</td>\n",
       "      <td>0.879143</td>\n",
       "      <td>06:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.336817</td>\n",
       "      <td>0.261022</td>\n",
       "      <td>0.917526</td>\n",
       "      <td>0.916494</td>\n",
       "      <td>06:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.314750</td>\n",
       "      <td>0.345615</td>\n",
       "      <td>0.891753</td>\n",
       "      <td>0.890587</td>\n",
       "      <td>06:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.290393</td>\n",
       "      <td>0.595228</td>\n",
       "      <td>0.798969</td>\n",
       "      <td>0.781551</td>\n",
       "      <td>06:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.230832</td>\n",
       "      <td>0.282863</td>\n",
       "      <td>0.896907</td>\n",
       "      <td>0.894873</td>\n",
       "      <td>06:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.196027</td>\n",
       "      <td>0.201220</td>\n",
       "      <td>0.927835</td>\n",
       "      <td>0.927863</td>\n",
       "      <td>06:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.125908</td>\n",
       "      <td>0.217970</td>\n",
       "      <td>0.943299</td>\n",
       "      <td>0.943269</td>\n",
       "      <td>06:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.140250</td>\n",
       "      <td>0.197226</td>\n",
       "      <td>0.938144</td>\n",
       "      <td>0.938122</td>\n",
       "      <td>06:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.137203</td>\n",
       "      <td>0.170899</td>\n",
       "      <td>0.958763</td>\n",
       "      <td>0.958457</td>\n",
       "      <td>06:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.130378</td>\n",
       "      <td>0.198441</td>\n",
       "      <td>0.943299</td>\n",
       "      <td>0.942868</td>\n",
       "      <td>06:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.106406</td>\n",
       "      <td>0.274372</td>\n",
       "      <td>0.917526</td>\n",
       "      <td>0.916748</td>\n",
       "      <td>06:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.142593</td>\n",
       "      <td>0.166811</td>\n",
       "      <td>0.953608</td>\n",
       "      <td>0.953608</td>\n",
       "      <td>06:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.128333</td>\n",
       "      <td>0.170706</td>\n",
       "      <td>0.927835</td>\n",
       "      <td>0.926872</td>\n",
       "      <td>06:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.102620</td>\n",
       "      <td>0.172565</td>\n",
       "      <td>0.938144</td>\n",
       "      <td>0.937701</td>\n",
       "      <td>06:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.083772</td>\n",
       "      <td>0.157044</td>\n",
       "      <td>0.953608</td>\n",
       "      <td>0.953467</td>\n",
       "      <td>06:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.058264</td>\n",
       "      <td>0.177544</td>\n",
       "      <td>0.953608</td>\n",
       "      <td>0.953242</td>\n",
       "      <td>06:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.041535</td>\n",
       "      <td>0.192523</td>\n",
       "      <td>0.958763</td>\n",
       "      <td>0.958378</td>\n",
       "      <td>06:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.045104</td>\n",
       "      <td>0.151028</td>\n",
       "      <td>0.958763</td>\n",
       "      <td>0.958691</td>\n",
       "      <td>06:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.041495</td>\n",
       "      <td>0.181798</td>\n",
       "      <td>0.958763</td>\n",
       "      <td>0.958263</td>\n",
       "      <td>06:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.038828</td>\n",
       "      <td>0.203504</td>\n",
       "      <td>0.943299</td>\n",
       "      <td>0.942827</td>\n",
       "      <td>06:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.030292</td>\n",
       "      <td>0.164590</td>\n",
       "      <td>0.938144</td>\n",
       "      <td>0.937976</td>\n",
       "      <td>06:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.048639</td>\n",
       "      <td>0.167039</td>\n",
       "      <td>0.958763</td>\n",
       "      <td>0.958638</td>\n",
       "      <td>06:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.051552</td>\n",
       "      <td>0.143854</td>\n",
       "      <td>0.958763</td>\n",
       "      <td>0.958669</td>\n",
       "      <td>05:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.052269</td>\n",
       "      <td>0.148951</td>\n",
       "      <td>0.953608</td>\n",
       "      <td>0.953572</td>\n",
       "      <td>06:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.037594</td>\n",
       "      <td>0.145802</td>\n",
       "      <td>0.958763</td>\n",
       "      <td>0.958691</td>\n",
       "      <td>06:13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with accuracy value: 0.6185566782951355.\n",
      "Better model found at epoch 1 with accuracy value: 0.7525773048400879.\n",
      "Better model found at epoch 2 with accuracy value: 0.7835051417350769.\n",
      "Better model found at epoch 3 with accuracy value: 0.8453608155250549.\n",
      "Better model found at epoch 4 with accuracy value: 0.8556700944900513.\n",
      "Better model found at epoch 5 with accuracy value: 0.907216489315033.\n",
      "Better model found at epoch 22 with accuracy value: 0.9175257682800293.\n",
      "Better model found at epoch 30 with accuracy value: 0.9278350472450256.\n",
      "Better model found at epoch 31 with accuracy value: 0.9432989954948425.\n",
      "Better model found at epoch 33 with accuracy value: 0.9587628841400146.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAm/ElEQVR4nO3deXhU5d3/8fc3KwmEACFhDUQ2kR1BRKyK2lZFRG2rtbX+tNra2qe1drHVPl2sbZ+2drer1L21VYu2pdYFqrgBImGVRZB9z74vk0zm/v0xExpjEgLkzPp5XddczJw5c+Z7M0k+c59z7vuYcw4REUlcSZEuQEREIktBICKS4BQEIiIJTkEgIpLgFAQiIgkuJdIFHK+BAwe6goKCSJchIhJT1qxZU+qcy+3ouZgLgoKCAgoLCyNdhohITDGzvZ09p11DIiIJTkEgIpLgFAQiIglOQSAikuAUBCIiCc7zIDCzZDNbZ2bPdPDcDWZWYmbrQ7dPeV2PiIi8WzhOH/0isBXo28nzTzjnPh+GOkREpAOe9gjMbDhwKXC/l+8jIhLvfvWfd3jtnRJPtu31rqFfAl8DAl2s82Ez22hmi8wsv6MVzOxmMys0s8KSEm/+I0REolWTP8AvX9zO6j0VnmzfsyAws/lAsXNuTRer/QsocM5NAZYCj3S0knNuoXNupnNuZm5uhyOkRUTi1sHKBpyDEQMyPdm+lz2Cs4EFZrYHeBy4wMz+3HYF51yZc84Xeng/MMPDekREYtK+8nogBoPAOXenc264c64AuAZ4yTn3ibbrmNmQNg8XEDyoLCIibXgdBGGfdM7M7gYKnXOLgVvNbAHgB8qBG8Jdj4hItNtfXk9aShJ5WemebD8sQeCcexl4OXT/222W3wncGY4aRERi1b6yevL7Z5CUZJ5sXyOLRUSi3L7yekbm9PZs+woCEZEo5pxjf3m9Z8cHQEEgIhLVKuubqfH5yVcQiIgkJq/PGAIFgYhIVNurIBARSWz7Q0GQPyDDs/dQEIiIRLF9ZfUM7JNOZpp3Z/srCEREoti+8npGeNgbAAWBiEhU2+fxqaOgIBARiVpN/gCHqxoUBCIiiepQZQMBh6djCEBBICIStcIxhgAUBCIiUetoEOQoCEREEtL+8nrSkpMYlNXL0/dREIiIRKl95fUMH+Dd9NOtFAQiIlEqHKeOgoJARCQqOefYV6YgEBFJWFUNwemnFQQiIgkqXKeOgoJARCQqhevUUVAQiIhEpdYgyO+vIBARSUjB6afT6J3u3fTTrRQEIiJRaF95vedzDLVSEIiIRKFwjSEABYGISNRpbglwqNL76adbKQhERKLM4crG4PTTYThQDAoCEZGoU1TTCMDgbG8nm2ulIBARiTJF1cEgyOubHpb3UxCIiESZomofgOfTT7dSEIiIRJnimkbSkpPol5kalvdTEIiIRJniah+5WemYeXsdglYKAhGRKFNc08igMB0fAAWBiEjUKar2kRem4wOgIBARiTrF1eoRiIgkrIamFqob/eT1VY9ARCQhFYcGk+VlxVGPwMySzWydmT3TwXPpZvaEme0ws1VmVuB1PSIi0ay4JjiGIN56BF8Etnby3E1AhXNuDPAL4MdhqEdEJGq1jiqOm2MEZjYcuBS4v5NVLgceCd1fBFxo4TpxVkQkChWHeVQxeN8j+CXwNSDQyfPDgP0Azjk/UAXktF/JzG42s0IzKywpKfGoVBGRyCsK86hi8DAIzGw+UOycW3Oy23LOLXTOzXTOzczNze2B6kREolNJmEcVg7c9grOBBWa2B3gcuMDM/txunYNAPoCZpQDZQJmHNYmIRLWimsawzTrayrMgcM7d6Zwb7pwrAK4BXnLOfaLdaouB60P3PxJax3lVk4hItCuq9oX1+ABEYByBmd1tZgtCDx8AcsxsB/Bl4I5w1yMiEk3CPaoYICUcb+Kcexl4OXT/222WNwJXhaMGEZFo19gc/lHFoJHFIiJRo/XU0XCOKgYFgYhI1Gi9VrF6BCIiCeroYLJ4OWtIRESOz9GL1sf7WUMiItKx4hofqclG/zCOKgYFgYhI1CiubiQvq1dYRxWDgkBEJGpEYlQxKAhERKJGcQRGFYOCQEQkahRVq0cgIpKwWkcVDwrzGAJQEIiIRIXWMQS5YR5VDAoCEZGo0HrRevUIREQSVFGERhWDgkBEJCpEalQxKAhERKJCpEYVg4JARCQqRGpUMSgIRESiQnGNLyJjCEBBICISFYqqG8N+QZpWCgIRkShQXOOLyKmjoCAQEYm4xuYWqhqaFQQiIomqpCZyo4pBQSAiEnGtYwjUIxARSVCto4p1sFhEJEFFcp4hUBCIiERcUXXkRhWDgkBEJOIiOaoYFAQiIhFXVNMYkVlHWykIREQirLjaF5FZR1spCEREIqyoWj0CEZGE1dAUvFZxXoTOGAIFgYhIREX61FFQEIiIRFRxTWQHk4GCQEQkoiI9vQQoCEREIiqSF61vpSAQEYmg4upG0lKSyM6IzKhiUBCIiERUcY2PvKz0iI0qBg+DwMx6mdmbZrbBzDab2Xc7WOcGMysxs/Wh26e8qkdEJBoFxxBE7vgAQIqH2/YBFzjnas0sFXjdzJ5zzr3Rbr0nnHOf97AOEZGoVVTdyKmDsyJag2c9AhdUG3qYGro5r95PRCQWRXp6CfD4GIGZJZvZeqAYWOqcW9XBah82s41mtsjM8jvZzs1mVmhmhSUlJV6WLCISNvVNfmp8fvIieMYQdDMIzKy3mSWF7o8zswWh3T1dcs61OOemAcOBWWY2qd0q/wIKnHNTgKXAI51sZ6FzbqZzbmZubm53ShYRiXrFraeOxkiP4FWgl5kNA5YA1wEPd/dNnHOVwDLg4nbLy5xzvtDD+4EZ3d2miEisi4bBZND9IDDnXD3wIeB3zrmrgIldvsAs18z6he5nAB8A3m63zpA2DxcAW7tZj4hIzCuqifxgMuj+WUNmZmcB1wI3hZYlH+M1Q4BHzCyZYOA86Zx7xszuBgqdc4uBW81sAeAHyoEbjrcBIiKxqjjUI4j0weLuBsFtwJ3A351zm81sFMFdPZ1yzm0Epnew/Ntt7t8Z2q6ISMIprvGRnpJE3wwvz+Q/tm69u3PuFeAVgNBB41Ln3K1eFiYiEu9aB5NFclQxdP+sob+YWV8z6w1sAraY2e3eliYiEt8ifWWyVt09WDzBOVcNXAE8B5xC8MwhERE5QcF5hiJ7fAC6HwSpoXEDVwCLnXPNaJSwiMhJKa72RXwwGXQ/CO4D9gC9gVfNbCRQ7VVRIiLxrtbnp9bnj/gYAuj+weJ7gXvbLNprZud7U5KISPwrPjqYLEZ6BGaWbWY/b53vx8x+RrB3ICIiJ+C/1yqOfI+gu7uGHgRqgKtDt2rgIa+KEhGJd0VR1CPo7iiG0c65D7d5/N3QrKIiInICWiecy4uCYwTd7RE0mNn7Wh+Y2dlAgzcliYjEv6LqRjJSk8lKj+yoYuh+j+CzwKNmlh16XAFc701JIiLxr7gmeOpopEcVQ/fPGtoATDWzvqHH1WZ2G7DRw9pEROJWUXVjxK9D0Oq4rlDmnKsOjTAG+LIH9YiIJITWHkE0OJlLVUa+PyMiEqNaJ5yLBicTBJpiQkTkBNT6/NQ3tZCXFR09gi6PEZhZDR3/wTcgw5OKRETiXLRcorJVl0HgnMsKVyEiIomiNQji4RiBiIicgNbBZNHSI1AQiIiE2aaDVQBRc4xAQSAiEkbLthXzwPLdXDFtKFm9UiNdDqAgEBEJmz2ldXzxr+sYP7gvP/zQlEiXc5SCQEQkDOp8fm7+UyFJScbC62aQkZYc6ZKOUhCIiHjMOcftizawo7iW33zsdPIHZEa6pHdREIiIeOyPr+3i2beOcMcl43nf2IGRLuc9FAQiIh7aU1rHT5ds56KJg/j0OaMiXU6HFAQiIh5xzvHNf2wiPTmJuy+fFBVTTndEQSAi4pHFGw7x+o5Sbr/41KgZPNYRBYGIiAcq65v43jNbmJrfj2vPHBnpcroU+WukiYjEoR8//zYV9c08cuMkkpOic5dQK/UIRER62Oo95fz1zf3c9L5TmDg0+9gviDAFgYhID/v+v7cyrF8Gt71/bKRL6RYFgYhID9pXVs+G/ZXcMKeAzLTY2PuuIBAR6UHPbToMwMWTBke4ku5TEIiI9KBnNx1hyvDsqJtGoisKAhGRHnKgIrhb6JJJQyJdynFREIiI9JDnNx0B4JIY2i0EHgaBmfUyszfNbIOZbTaz73awTrqZPWFmO8xslZkVeFWPiIjXnt90hNOG9KVgYO9Il3JcvOwR+IALnHNTgWnAxWY2u906NwEVzrkxwC+AH3tYj4iIZ45UNVK4t4J5MdYbAA+DwAXVhh6mhm6u3WqXA4+E7i8CLrRonZVJRKQLL2wO7RaaHFvHB8DjYwRmlmxm64FiYKlzblW7VYYB+wGcc36gCsjpYDs3m1mhmRWWlJR4WbKIyAl59q3DjBvUhzF5fSJdynHzNAiccy3OuWnAcGCWmU06we0sdM7NdM7NzM3N7dEaRUSO1xu7yqhqaD76uKTGx+o95TF3tlCrsAx7c85Vmtky4GJgU5unDgL5wAEzSwGygbJw1CQiciKW7yjl2vtXkZWewnVnjeSm953Cki1HCDi4ZHLsHR8AD4PAzHKB5lAIZAAf4L0HgxcD1wMrgY8ALznn2h9HEBGJGo+u3EP/zFTmjBnI71/ZyYPLd5Odkcqogb05dVBWpMs7IV7uGhoCLDOzjcBqgscInjGzu81sQWidB4AcM9sBfBm4w8N6REROyuGqBpZuKeKjZ4zgtx8/naVfOo95k4dQWtvEldOHRe0VyI7FYu0L+MyZM11hYWGkyxCRBPTzJdv49bIdvHr7+e+aQqK6sZneaSlRfd0BM1vjnJvZ0XOxMTWeiEiENfkD/HX1fs4/Ne898wj17ZUaoap6hqaYEBHphiVbjlBS4+O62dF92ckToSAQEemGP63cS/6ADM4dF3+nsCsIRESOYXtRDat2l/OJM0dG9XGAE6UgEBE5hj+/sZe0lCSumpkf6VI8oSAQEelCrc/P02sPMn/KEAb0Tot0OZ5QEIiIdOHZjYep9fm59sz4O0jcSkEgItKFZ946TP6ADE4f0S/SpXhGQSAi0onK+iZW7Chl3uQhMTtquDsUBCIinViyuQh/wHFpDF5j4HgoCEREOvHvtw4zvH8Gk4dlR7oUTykIREQ6UFnfxPIdpVwa57uFQEEgItKhJVuCu4XmxfluIVAQiIh06NnQbqEpw+N7txAoCERE3qOqvpnlCXC2UCsFgYhIO0u2HKG5JTF2C0ECXY/A52+httFPTp/0SJcSk5xzlNT42HqkhrcPV7O9qJYRAzK5eNJgxg3q0+Pfmup8fr77r80UDOzNp943irQUfWeR8Hn2rcMM65fB1ATYLQQJFASvbCvhlsfWctaoHOZPGcJFEwfTv4fnDWluCfDKthKmjejHwDgKnLX7KvjCX9ZxsLLh6LKBfdIpq/Pxi/9spyAnk4smDWbepCFMGZ590qFQUuPjxodXs+lQFc7BP9cd4ocfnszpI/qfbFNEjqmqoZnXd5TyybNPSYjdQpBAl6rcW1bHk4X7eWbjYfaW1ZOcZJwzdiD3fHgKeX179Uhtv122g5+8sI0kg5kjB/DBiYO4aOLg91zNKJY8v+kwX3x8PYOze/HJOQWMH9KX8YOz6JeZRnFNI0u3FPH8piOs3FmGP+AYMSCT+VOGcNnUoYwfnHXcv0h7Suu4/qE3Kapu5HfXnk5LAL79z00cqW7kutkj+epFp8b81aAkuv2tcD+3L9rIP/7nbKbl94t0OT2mq0tVJkwQtHLOsflQNc9sPMwDr+/i2jNHcteCiSddV0VdE+fes4yp+f04fWR/lmw+wttHagD44IRBfOWDp3Lq4KyTfp9weuD13Xz/31uYlt+P+//fzC53q1XVN/PCliP8a8MhVuwsoyXgGJLdi4Kc3owYkMmInEzyB2QyvH8Gw/tnkNsn/V0h4Zxj44Eqbnx4NQHnePCGM5ge6gHU+vz89IVtPLJyD+kpScwdl8clkwdz4WmD6JOeMJ1aCQPnHAt+s5w6n58Xv3JeXPUIFASduO3xdby4tZiV37jwpP+g/PDZrSx8bRcv3HYu4wYF/+DvLavj7+sO8sBru6lt8nP51KHc9v5xjMzJpKK+mcNVDRypamTcoKyo6jX4WwL84NmtPLR8DxdPHMwvr5lGr9Tkbr++rNbHs5uOULinnP3l9ewrb6C01veuddJTkhjUtxfNLQHqfH7qm1rwBxzD+2fw6I2zGJXb5z3b3XSwiicL9/PcpuAlA9NSkjijoD8FOb0ZmZPJyJzeDOuXQWZaMr1Sk8lITSYzPZn0lO7XLontjV1lXLPwDX5w5aS4m21UQdCJtfsq+NDvVvC9Kyad1HVID1c1MPcnLzN/ylB+dvXU9zxfWd/Efa/u4qHlu2lucSQnGU3+wNHne6Um8Z3LJnLNGfnH/Q2k1ufn+U1HOGVgJtPy+5/01ZNW7CzlrsWb2V5Uy03vO4VvzDutR67IVN/kZ395Awcr6zlQ0cCBigaKqhtJT0kiMy2F3unJ9O2VyodOH05uVtfHVwIBx9p9Ffz7rcOs2VvB3rJ6qhqaO1w3Ocn4wGmDuH5OAbNHDYirb3jS8256eDXr9ley4o4LjuvLTyxQEHSitRvY2NzCki+de8J/JO58eiOL1hzgpa/M7fKbfXFNI4+u2EtzS4DB2b0Ykt2L/plp3PvSOyzfUca8yYP54ZVTyM5MPVrfzpJaiqp9TM3v965ei8/fwmNv7OO3y3ZQVtcEQP/MVOaemscF44O33sfRyzlY2cD//Xvr0blVvjV/AhdNHHxC/x+RUFXfzN7yOg5VNtLY3EJDcwuNzS0cqGjgqbUHqKxv5tRBWVx31khOG9KXrF4p9ElPOfp/5PO34GsO4PO3kJvVi+wMHYdINDuKa3j/z1/ltveP5bb3j4t0OT1OQdCFRWsO8NW/beAvnzqTOWMGHvfrd5bU8sFfvMp1s0/8WEMg4Ljv1V38bMk2BvXtxcfPHMHGA5UU7qk4+kc+OcmYOjybOaMHkpuVzsJXd3GwsoE5o3O49cKxlNb6eGlrMcu2FVNR30xWegpXn5HP9WcVMCKn43DaX17P8h2lrNhZxtItRQSc43Nzx/CZ80bF1behxuYWFm84xCMr9rD5UPUx109PSeLSKUO49syRnD6in3oRCeLOpzfy9NqDrLjjgrg8zVxB0IXG5hbO+uGLzDplAPdd1+H/UZf+57G1LNtWzKtfO/+kTxldv7+SLz6+jr1l9eQPyGBWQQ6zTunPoL69KNxTwfKdpWw8UEVLwDF5WDZfv3g87xv77vBqCTjW7K3gz2/s5dm3DtPiHBeOH8T0Ef2orG+ivK6ZivomthfVcKAieDpoblY6c8flcuuFY6PqWEVPc87x9pEaiqobqfO1UOtrpqbRj5mRnpJEekoSaSlJrN5Tzj/WHaLW52f84Cw+fPpwZhT0Z+LQvjreEKdKanyc/eOX+MiM4fzflZMjXY4nFATH8OPn3+a+V3by2tcvYFi/jKPLN+yvJDnJmNTJFLSvbC/h+gff5NYLx/LlD/RMV7LJH6CqobnT/eQ1jc3sL2/gtCHHPjXzSFUjj63ay19W7aOsromM1GQG9E6jX2Yqw/tnMGf0QOaMzmFMXs8PCIt1dT4/izcc4rFVe9l0MNiLSE02JgzNZs7oHL544di46jUlup8v3c69L77Di185j9EdnKgQDxQEx3Cgop5z71nGZ84bzdcvHk9prY/vPbOFf64/BMDZY3L43NwxzBmdg5mxZm8Fv3rxHV7dXsKwfhk8f9s5ZEXxue3+lgD+gNMfrhNUVN3Iun2VrN9fybp9FazaXc6543JZeN0M/Z/GgYamFub86EVmjBzA/dcf/16BWNFVEOgkbGB4/0w+MGEQj7+5j+H9M7jn+W3UN/m59YIx9OmVwh9f2821969ian4/+vZK4bV3ShnQO407LhnPdbNHHtdB2UhISU5CezRO3KC+vbh40mAunhQ8eP7k6v18/emNfOqRQv74/2aSkab/3FgVCDj++NouKuqbufncUZEuJ2LUIwhZsaOUj9+/CoBZBQP4vw9NYkxecDxAY3MLT609wH2v7KLW5+fmc0fFRACIdxatOcDtizYw+5QcHrhhJplp+lmIJS0Bx3ObDvPrF3ewraiGc8YO5NEbZ8X1LlLtGuoG5xw/eu5tRuf24SMzhpPUwbnzzrm4/kGR4/P3dQf4ypMbmDlyANfPKWB0Xm8Kcnprd1GUW7L5CPe8sI0dxbWMyevDFy4Yw/wpQ3tkvEw0UxCIeOSf6w9y+6KNRwcImkF+/0w+f8EYrp6ZH+HqpK1AwPGzpdv47bKdjBvUh1svHMslk4bEfQC00jECEY9cPm0YH5wwmF2ltewqqWNnSS2vvVPK1xZtZMuhar556WmkJGsK7Uir8/m57Yn1LN1SxDVn5HP35ZM0tXkbCgKRk5SRlszEodlMHBo8zfjz54/hh8+9zQOv72ZHcS2/+fh0+mX27JTn0n37y+v59KOFbC+q4TuXTeCGOQXaxduOIlGkh6UkJ/Gt+RO45yNTeHN3OZf/djlbujGiWXre3rI6rvzdCg5WNvDwJ2cl1DUGjoeCQMQjV8/M5683z6bO18Jlv3mduxZvpqq+48nxpOeV1fq4/sE3aQkEePqWOZw7LjfSJUUt7RoS8dCMkf1Z+qVz+dnSbTy6cg+LNxziaxedymVTh3KosoF95fXsK68np086l01JjAulh0NDUwuferSQw1WN/OXTZzJ2UGxdCyTcPDtryMzygUeBQYADFjrnftVunbnAP4HdoUVPO+fu7mq7OmtIYtXmQ1V8d/EW3txT3uHznz1vNF+/+FSFwUlqCTg+++c1/GdrEb+/dsbRgYCJLlJnDfmBrzjn1ppZFrDGzJY657a0W+8159x8D+sQiQoTh2bzxGdm8/ymI+wsqSV/wH+v2nbvi+/wh1d20twS4JuXnqYwOEHOOb6zeBNLtxTx3QUTFQLd5FkQOOcOA4dD92vMbCswDGgfBCIJw8y4ZPKQ9yz/3uWTSE1O4oHXd9PcEuCuyyZ2OKhROre3rI47nnqLlbvK+My5o7h+TkGkS4oZYTlGYGYFwHRgVQdPn2VmG4BDwFedc5s7eP3NwM0AI0aM8LBSkcgwM749fwKpyUksfHUXjc0t3HnJafTvrdNOj6Ul4Hjw9d38bOk2UpOS+MGVk/j4LP2dOB6ejyw2sz7AK8APnHNPt3uuLxBwztWa2TzgV865sV1tT8cIJJ455/j50u38+qUdpKUkMX/KED4xeyTT8+PzAjmBgMOMY7YtEHC8+k4JDy3fw9p9FfTPTGNgnzQG9knnYGUDmw9Vc+H4PL5/5SSGZGd0ua1EFbEpJswsFXgGeME59/NurL8HmOmcK+1sHQWBJIKth6t5bNVe/r72IHVNLUwa1pd7r5nOqDiaK3/TwSo++fBqks04e8xAzh6Tw9ljBpKXlU5jc4C6Jj/1vhZeeaeEh5fvZmdJHblZ6XxgwiBqG/2U1voorfXhDzi+eOFYFkwdGpdh2VMiEgQW/EQeAcqdc7d1ss5goMg558xsFrAIGOm6KEpBIImk1ufnH+sO8oul28lIS+bpz80hL6tXpMs6aduLavjofSvJTEthWn4/VuwspSI0xsIM2v8FmDI8mxvPPoV5k4doaogTFKmzhs4GrgPeMrP1oWXfAEYAOOf+AHwEuMXM/EADcE1XISCSaPqkp/CJ2SOZMjybaxa+wScfWs3jN8+O6gshHcvu0jquvX8VqclJ/OXTZzIypzeBgGPL4WpW7iyjprGZjLQUeqcnk5GazJi8PkyL011j0UKzj4rEiJe3FXPTI4WcNSqHB284Iya/GR+oqOfqP6yk0R/giZtna6BXGHXVI4i9nySRBDX31Dx+9KHJvL6jlK8/tZFY+RJX3+Rnxc5Sfv3iO1yz8A1qfX7+dNMshUAU0RQTIjHkqpn5FFU38tMl2zlQUc/nzh/D3HG5UbnbZNPBKr75j028dbCKlkAwtMYPzuLXH5t+dKZWiQ4KApEY8z/njyE7M43fLdvBJx9azYQhfbll7mjmTY6ei6wcqWrkpkdWA/DZ80Yxc+QATh/Rn+zM2D22Ec90jEAkRjX5A/xj/UH+8MpOdpXUMX5wFj+4chIzRg6IaF0NTS1cfd9KdpXU8tTn5jB+cN+I1iNBOkYgEofSUpK4emY+S790Hr/+2HSqGpr58O9XcsdTG6moa4pITYGA46t/28CmQ1X86prpCoEYoV1DIjEuOcm4bOpQLhifx70vvsMDr+/mhc1HuPvySVw2dWiPv19JjY8XNh/h+U1HaGhuYc7oHM4Zm8v0Ef34zUs7+Pdbh/nGvPG8f8KgHn9v8YZ2DYnEmW1Harjz6Y2s21/JLz86jcunDeuR7T6z8RCPrtzL6j3lOAejBvYmOzOVDfsrCTjonZZMXVMLV80Yzj0fmRKVB7ATmS5eL5JATh2cxV8+PZsbHnqTLz+5gYzUZD448eSmY/77ugN86YkNjMrtza0XjGXe5CGMG9QHM6OqoZmVO8t4fUcJzX7H3VdMVAjEGPUIROJUrc/PJ+5fxZZD1Txww0zOGRu8VGNjcwsvbytmzd4K+mWmkZuVTl5WOkP7ZTA2r897/oi/9k4Jn3xoNWcUDODhG88gPSU5Es2RkxSxSee8oCAQ6b6q+mY+unAle8vq+calp7FuXwVLNhdR6/OTmmw0t7z79//MUwbwrfkTmDQseJ7/poNVfPS+leQPyOTJz55F3xie2iLRKQhEElhJjY+P3reSXaV1ZPVK4ZJJg7ls6lDOGpVDc4ujtNZHcU0jG/ZX8dtlOyivb+LK6cP4+KwR3PLYWtKSk3jqljkMzo79ye4SmYJAJMGV1vrYcqiaM0cN6HLXTnVjM79btpMHl++myR8gOyOVp245izF5mg4i1ikIROS47C+v54HXd3P5tKFMH9E/0uVID9BZQyJyXPIHZHLXgomRLkPCRCOLRUQSnIJARCTBKQhERBKcgkBEJMEpCEREEpyCQEQkwSkIREQSnIJARCTBxdzIYjMrAfa2W5wNVB1jWVePW++3XTYQKD2JUjuqqbvr9FR72t6P9va0XxZr7eloeay0p7Pn1J74as9I51xuh1t3zsX8DVh4rGVdPW69325ZYU/X1N11eqo97doW1e3pThuiuT0n8plES3u6+xmpPbHfns5u8bJr6F/dWNbV4391ss7J6M62Olunp9rT3Tq6w+v2tF8Wa+3paHmstKez59Se+GtPh2Ju11C4mFmh62SCplik9kQ3tSe6xVt72ouXHoEXFka6gB6m9kQ3tSe6xVt73kU9AhGRBKcegYhIglMQiIgkuIQIAjN70MyKzWzTCbx2hpm9ZWY7zOxeM7M2z33BzN42s81mdk/PVt1lTT3eHjO7y8wOmtn60G1ez1feaU2efD6h579iZs7MBvZcxcesyYvP53tmtjH02Swxs6E9X3mnNXnRnp+Efnc2mtnfzaxfjxfeeU1etOeq0N+BgJnF3kHlkzk3NlZuwLnA6cCmE3jtm8BswIDngEtCy88H/gOkhx7nxXh77gK+Gi+fT+i5fOAFggMQB8Zye4C+bda5FfhDjLfng0BK6P6PgR/HeHtOA04FXgZmhqstPXVLiB6Bc+5VoLztMjMbbWbPm9kaM3vNzMa3f52ZDSH4C/iGC37ajwJXhJ6+BfiRc84Xeo9iTxvRhkftiRgP2/ML4GtAWM+I8KI9zrnqNqv2Joxt8qg9S5xz/tCqbwDDPW1EGx61Z6tzblsYyvdEQgRBJxYCX3DOzQC+Cvyug3WGAQfaPD4QWgYwDjjHzFaZ2Stmdoan1R7bybYH4POhrvqDZhbpK5afVHvM7HLgoHNug9eFdtNJfz5m9gMz2w9cC3zbw1q7oyd+3lrdSPDbdST1ZHtiTkJevN7M+gBzgL+12aWcfpybSQEGEOwmngE8aWajQt8UwqqH2vN74HsEv2l+D/gZwV/QsDvZ9phZJvANgrsfIq6HPh+cc/8L/K+Z3Ql8HvhOjxV5HHqqPaFt/S/gBx7rmepOqIYea0+sSsggINgTqnTOTWu70MySgTWhh4sJ/nFs22UdDhwM3T8APB36w/+mmQUITkxV4mHdnTnp9jjnitq87o/AMx7Weywn257RwCnAhtAv9nBgrZnNcs4d8bb0DvXEz1tbjwHPEqEgoIfaY2Y3APOBCyPxBaqNnv58Yk+kD1KE6wYU0ObgELACuCp034Cpnbyu/cGheaHlnwXuDt0fB+wnNEAvRtszpM06XwIej+XPp906ewjjwWKPPp+xbdb5ArAoxttzMbAFyA1nO7z+eSNGDxZHvIAwfeh/BQ4DzQS/yd9E8Bvj88CG0A/ktzt57UxgE7AT+E3rH3sgDfhz6Lm1wAUx3p4/AW8BGwl++xkSy+1pt05Yg8Cjz+ep0PKNBCcRGxbj7dlB8MvT+tAtnGdBedGeK0Pb8gFFwAvhak9P3DTFhIhIgkvks4ZERAQFgYhIwlMQiIgkOAWBiEiCUxCIiCQ4BYHEBTOrDfP7reih7cw1s6rQrKJvm9lPu/GaK8xsQk+8vwgoCEQ6ZGZdjrp3zs3pwbd7zQVHtU4H5pvZ2cdY/wpAQSA9RkEgcauzGSXN7LLQZIHrzOw/ZjYotPwuM/uTmS0H/hR6/KCZvWxmu8zs1jbbrg39Ozf0/KLQN/rH2sxRPy+0bE1o7voup+1wzjUQHFzVOnHep81stZltMLOnzCzTzOYAC4CfhHoRo7szc6ZIVxQEEs86m1HydWC2c2468DjBqapbTQDe75z7WOjxeOAiYBbwHTNL7eB9pgO3hV47CjjbzHoB9xGcr34GkHusYkMzvo4FXg0teto5d4ZzbiqwFbjJObeC4Mjv251z05xzO7top0i3JOqkcxLnjjGj5HDgidD88mnA7jYvXRz6Zt7q3y54zQmfmRUDg3j3VMQAbzrnDoTedz3BeWxqgV3OudZt/xW4uZNyzzGzDQRD4JfuvxPjTTKz7wP9gD4EL7JzPO0U6RYFgcSrDmeUDPk18HPn3GIzm0vw6myt6tqt62tzv4WOf2e6s05XXnPOzTezU4A3zOxJ59x64GHgCufchtBMnXM7eG1X7RTpFu0akrjkglf02m1mVwFY0NTQ09n8d/rg6z0qYRswyswKQo8/eqwXhHoPPwK+HlqUBRwO7Y66ts2qNaHnjtVOkW5REEi8yDSzA21uXyb4x/Om0G6XzcDloXXvIrgrZQ1Q6kUxod1LnwOeD71PDVDVjZf+ATg3FCDfAlYBy4G326zzOHB76GD3aDpvp0i3aPZREY+YWR/nXG3oLKLfAu84534R6bpE2lOPQMQ7nw4dPN5McHfUfZEtR6Rj6hGIiCQ49QhERBKcgkBEJMEpCEREEpyCQEQkwSkIREQS3P8HBeRAsXGF3oAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#make sure you are using a gpu\n",
    "defaults.device = torch.device('cuda')\n",
    "\n",
    "\n",
    "trans_model= cnn_learner(data, models.vgg16_bn, metrics=[accuracy,FBeta(average=\"weighted\")])\n",
    "\n",
    "\n",
    "\n",
    "trans_model.unfreeze()\n",
    "\n",
    "\n",
    "trans_model.lr_find()\n",
    "trans_model.recorder.plot()\n",
    "#Train again\n",
    "trans_model.fit_one_cycle(epochs,callbacks=[SaveModelCallback(trans_model, every='improvement', mode = 'max', monitor='accuracy', name=save_loc)])\n",
    "\n",
    "\n",
    "trans_model.save(save_loc)\n",
    "trans_model.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
